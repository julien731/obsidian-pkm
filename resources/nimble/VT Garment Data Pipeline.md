---
type: resource
migrated_from: notion
migrated_at: 2026-01-12
created: 2025-10-09
---

# VT Garment Data Pipeline

# Plan

## **ðŸ§© Phase 1:**

## **Wrangle What You Know**

You donâ€™t need to catalog every table in the DB. You need to **create a usable subset** â€” a tidy version of the useful data.

---

### **âœ… Step 1:**

### **Build a Mini Data Model**

You already know the â€œgoodâ€ tables (production, staff, issues, etc.). Use those and ignore the rest like the unindexed, forgotten dreams they are.

Create a clean diagram or list with:

- Table name (actual DB name)
- Business meaning (human speak)
- Join relationships (e.g., jobtran.emp_num â†’ employee.emp_num)
- Rough row count (use COUNT(*))
- Date coverage (MIN(date), MAX(date))

**Deliverable:** A focused, documented mini-model of the **core production dataset**.

---

### **âœ… Step 2:**

### **Flatten the Data for LLM Consumption**

LLMs donâ€™t want raw SQL tables. They want:

- Fewer joins
- Fewer nulls
- More context per row
- Less entropy

So you:

- **Create wide tables** that consolidate data. Examples:
    - DailyEmployeeProductionPerformance
    - MachineDowntimeEvents
    - ProductionLineDailySummary
- Include all your useful features:
    - emp_num, style, hours_worked, qty_complete, downtime_minutes, otp, efficiency, rework_rate, etc.

Use **views or temp tables** to prep this stuff. You are building **narrative tables**, not just relational ones.

**This is the LLM food.**

---

### **âœ… Step 3:**

### **Give Columns Human-Readable Aliases**

LLMs donâ€™t like Uf_QtyPerson, cjr.oper_num, m0.description.

Create **aliases**:

- QtyPerPerson
- OperationNumber
- CustomerDescription

And save a **column glossary** alongside the table, e.g., as JSON or CSV.

> The more human it looks, the less the LLM screams internally.
> 

---

## **ðŸ§  Phase 2:**

## **Build a Data-to-Text Pipeline**

Your eventual goal is to ask something like:

> â€œShow me the most common root causes of downtime for Line 7 over the past 3 monthsâ€
> 

Which means the LLM will need:

1. A structured format to **understand** the data
2. A way to **retrieve** subsets (or get summaries)
3. Enough **context** to not hallucinate like a drunk oracle

Hereâ€™s the plan:

---

### **ðŸ”„ Step 4:**

### **Format Data for Context Windows**

Depending on your LLM input limits:

- Use **CSV/JSON** for flat data chunks
- Use **Markdown tables** if youâ€™re summarizing
- Use **database connectors or retrieval-augmented generation (RAG)** if youâ€™re using embeddings

**Chunking strategies:**

- Split by time: â€œLast 30 daysâ€, â€œMonth-by-monthâ€
- Split by worker/line/machine
- Summarize by metric: avg, sum, anomalies

If youâ€™re prepping this for an **offline prompt or static ingestion**, youâ€™ll want to:

- Summarize per row (or group)
- Annotate the summaries
- Include units, explanations

Example LLM-ingestible row:

```json
{
  "date": "2025-10-01",
  "line": "7SEW01",
  "employee": "ASEW024",
  "hoursWorked": 7.5,
  "qtyProduced": 120,
  "efficiencyPercent": 84.2,
  "mainDowntimeReason": "Material shortage",
  "reworkRatePercent": 3.2
}
```

### **ðŸ§  Step 5:**

### **Consider Using Vector Embeddings for Long-Term Queries**

If youâ€™re actually building an LLM-powered dashboard or assistant:

- Vectorize your **table-level summaries**, column glossaries, and metadata
- Use a RAG setup to allow the LLM to search your indexed data model and answer questions
- Store embeddings of:
    - Descriptions of downtime types
    - Field definitions
    - Common SQL queries

**ðŸ”¥ Bonus: Tools You Should Consider Using**

| **Tool** | **Purpose** |
| --- | --- |
| **SQLAlchemy or dbt** | For transforming and managing clean data models |
| **Pandas** | For feature engineering and flattening |
| **DuckDB** | If you want to process big data on your laptop like a wizard |
| **LangChain / LlamaIndex** | For feeding data into an LLM via RAG |
| **Airbyte/Fivetran** | If you want to automate pulling from the source database |
| **Retool/Streamlit** | For building interfaces on top of this once itâ€™s ready |

## **ðŸš¨ Pitfall Warnings**

- **Do not** shove 10 years of raw jobtran data into a prompt. It will cry. You will cry.
- **Do not** expect the LLM to guess column meanings. Be explicit.
- **Do not** rely on the Excel doc as your data dictionary. Bake definitions into the data.

# Research

## **ðŸ¤¨ â€œWait, why not just use SQL Server views?â€**

Because **views alone are like duct-tape and hope** â€” they technically work, but they donâ€™t scale, document, or explain themselves. Theyâ€™re the *raw* version of what **dbt gives you with some structure, repeatability, and sanity control**.

| **Feature** | **SQL Server Views** | **dbt Models** |
| --- | --- | --- |
| âœ… Can create views | âœ”ï¸ | âœ”ï¸ (or materialized tables) |
| ðŸ”„ Version control | âŒ (unless you wrap your .sql files in Git and pray) | âœ”ï¸ Built-in with Git + file structure |
| ðŸ§¾ Inline documentation | âŒ (only via SQL comments, nobody reads them) | âœ”ï¸ YAML-based docs shown in UI |
| ðŸ§ª Testing & validation | âŒ | âœ”ï¸ dbt test supports unique, not null, relationships, custom tests |
| ðŸ” Data lineage / DAG | âŒ | âœ”ï¸ Visual lineage of how models build on each other |
| ðŸ“¦ Layered architecture (staging, intermediate, marts) | âŒ (you *can* do it, but thereâ€™s no structure) | âœ”ï¸ Convention over chaos |
| ðŸ“Š Works with BI tools | âœ”ï¸ | âœ”ï¸ (output is just tables/views in your DB) |
| ðŸ§  Standardized patterns for transformations | âŒ | âœ”ï¸ (e.g., macros, reusable logic) |
| ðŸ› ï¸ Easily refactorable | âŒ | âœ”ï¸ Modular, readable code by layer |
| ðŸš¨ CI/CD friendly | âŒ | âœ”ï¸ Integrates with GitHub Actions, etc. |
| ðŸ¤ Team collaboration | âŒ (unless your team loves naming things view_v6_newNEW.sql) | âœ”ï¸ Git + docs + shared models = teamwork dreamwork |

## **The Minimal Viable Data-to-LLM Pipeline**

Hereâ€™s the diagram first, then Iâ€™ll break it down:

```
[SQL Server] 
   â¬‡ (raw data)
[dbt models]  â† versioned, tested, documented transformations
   â¬‡
[Transformed Tables / Views] (e.g., fct_downtime_summary, dim_employee)
   â¬‡
[Pandas / ETL Script] â† load, flatten, clean
   â¬‡
[LLM Interface] (OpenAI API / LangChain / Chatbot / Dashboard Assistant)
```

Now letâ€™s walk through the pieces like a normal person trying not to scream.

---

### **ðŸ§© Step 1:**

### **SQL Server (Raw Data)**

- This is where all the horror lives: jobtran, item, downtime_code, etc.
- Do **not** connect your LLM directly to this. Unless you want it to develop sentience and cry.

---

### **ðŸ§© Step 2:**

### **dbt (Transformation & Lineage)**

Use dbt to:

- Clean cryptic fields (Uf_StopHour â†’ OvertimeStopMinutes)
- Rename and reformat
- Create layered models:
    - stg_jobtran.sql (raw cleanup)
    - int_employee_downtime.sql (calculations)
    - fct_downtime_summary.sql (final wide table)

You now have **LLM-ready tables**. Use materialized: table or view as needed.

---

### **ðŸ§© Step 3:**

### **Transformed Tables in SQL Server**

Example: fct_downtime_summary

| **date** | **emp_id** | **line** | **total_downtime** | **top_reason** | **rework_rate** |
| --- | --- | --- | --- | --- | --- |
| 2025-09-01 | VSEW13 | 7SEW01 | 22 | MM003 | 3.2 |
| â€¦ | â€¦ | â€¦ | â€¦ | â€¦ | â€¦ |

This is your **analysis layer**, shared between:

- Power BI
- LLM (via Pandas or LangChain)

---

### **ðŸ§© Step 4:**

### **Pandas / Python Script (ETL for LLM)**

Use Python to:

- Load transformed tables:

```python
import pandas as pd
import pyodbc

conn = pyodbc.connect("DRIVER={ODBC Driver 17 for SQL Server};SERVER=...;DATABASE=...;UID=...;PWD=...")
df = pd.read_sql("SELECT * FROM fct_downtime_summary", conn)
```

- Optional: Chunk/summarize by date, line, employee
- Output as:
    - **JSON** (for prompt-based LLMs)
    - **Markdown table** (for small summaries)
    - **CSV or Parquet** (if feeding into a vector DB or RAG pipeline)

---

### **ðŸ§© Step 5:**

### **LLM Interface**

You now have a few routes:

### **a. Prompt-based LLM (OpenAI API)**

Use the cleaned data in a prompt like:

> â€œHere is a table of employee production and downtime records. Based on this, summarize the top issues and suggest improvements.â€
> 

Paste in a few rows from df.head(10).to_markdown(), or summarize first.

### **b. RAG (Retrieval-Augmented Generation)**

Embed the cleaned dataset using tools like:

- **LangChain**
- **LlamaIndex**
- **Pinecone / FAISS / Chroma** for embeddings

Then allow queries like:

> â€œWhat patterns of downtime occur on Line 7 over time?â€
> 

> â€¦and the system will:
> 
- Search relevant rows or summaries
- Feed into the LLM for final natural language response

### **c. Chatbot or Dashboard Assistant**

Hook the LLM up to a Streamlit app or Power BI plugin, allowing natural-language Q&A over the clean dataset.

## Terms & Definitions

| **Layer** | **Tool** | **Purpose** |
| --- | --- | --- |
| Data Storage | SQL Server | Store raw & transformed tables |
| Transformation | dbt + SQL | Clean, document, version |
| Interface Layer | Pandas + Python | Load data, flatten, clean |
| AI Layer | OpenAI / LangChain | Analyze, summarize, suggest |
| Output | Chat, CLI, Dashboard | Interact, visualize, question |

# Project Plan

| Activity | Description | Duration |
| --- | --- | --- |
| Deep dive into the DB | - Identify all relevant tables
- Document each table & **each field** |  |
| Create dbt models & schemas | First stage of preparation for LLM ingestion.

- Create clean data models
- Standardize data types
- Document the whole schemas
- Document data relationships |  |
| Define relevant KPIs & views | - Add logic to generate actionable data for analysis |  |
|  |  |  |